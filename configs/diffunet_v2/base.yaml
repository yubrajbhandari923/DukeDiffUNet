experiment:
  name: "DiffUNet_v2"
  version: 2.0
  description: "DiffUNet for generative colon conditioned on surrounding organs"
  tags: ["DiffUNet", "3D VLSM"]
  debug: false

seed: 42

logging:
  use_aim:       true
  aim_repo:      "/home/yb107/cvpr2025/DukeDiffSeg/aim/"

data:
  description:        "3d VLSM dataset for colon segmentation given by Lavsen, but with new combined labels with 22 classes"
  num_classes:        14     # background + 13 organs
  colon_label:        1     # Label for colon segmentation
  # cache_dir:          "/NAS/user_data/user/yb107/cache_dir"
  cache_dir:          "/data/usr/yb107/colon_data/cache_v2"
  orientation:        "RAS"
  pixdim:             [1.5, 1.5, 2.0]
  roi_size:           [96, 96, 96]
  # roi_size:           [128, 128, 128]
  # roi_size:           [192, 192, 192]
  # roi_size:           [256, 256, 256] # Adjusted for colon segmentation
  slice_axis:         -1
  
  train_jsonl:        "/home/yb107/cvpr2025/DukeDiffSeg/data/3d_vlsm/3d_vlsm_train.jsonl"
  batch_size_per_gpu:         1
  num_workers_per_gpu:        4
  shuffle_train_data: true

  val_jsonl:          "/home/yb107/cvpr2025/DukeDiffSeg/data/3d_vlsm/3d_vlsm_val.jsonl"
  val_batch_size:    1
  val_num_workers:    4

  debug_save_data: false  # Save a few samples for debugging

# ───────────────────────────────────────────────────────────
# Training hyperparameters
training:
  device:              "cuda"
  epochs:              5000
  accumulate_grad_steps: 2
  save_dir:            "/home/yb107/cvpr2025/DukeDiffSeg/outputs"
  save_interval:       5      # epochs
  resume:              null # Path to checkpoint to resume from (set to null to not resume)
  num_gpus:          1      # Number of GPUs to use for training
  
# ───────────────────────────────────────────────────────────
# Optimizer & scheduler
optimizer:
  name:       "AdamW"
  lr:         2e-4
  weight_decay: 1e-3
  set_to_none: true  # Set optimizer parameters to None for memory efficiency

ema:
  enable: true
  ema_rate: 0.9999

lr_scheduler:
  # pick either LinearWarmupCosineAnnealingLR or any torch scheduler
  name:               "LinearWarmupCosineAnnealingLR"
  # only used by the warmup+cosine scheduler:
  warmup_epochs:      100
  max_epochs:         ${training.epochs}

# ───────────────────────────────────────────────────────────
# Evaluation & early stopping
evaluation:
  validation_interval:     40        # epochs
  validation_max_num_samples: 80
  metrics:               ["Mean Dice"]
  early_stopping:
    enabled: true
    patience: 20
  visualize:             true
  visualize_every_iter:  1 

# ───────────────────────────────────────────────────────────
# Model architecture (easy to swap out a different 'model.params' block)
model:
  name: "DiffUNet"
  params: 
    spatial_dims: 3
    in_channels:  ${constraint.model.params.in_channels} # Overridden 
    out_channels: ${task.model.params.out_channels} # Overridden 
    features: [32, 64, 128, 256, 512, 64]
    activation: ["LeakyReLU", {"negative_slope": 0.1, "inplace": False}]
    beta_schedule: "linear"

    dropout: 0.0
    use_checkpointing: true

# ───────────────────────────────────────────────────────────
# Diffusion-specific hyperparameters
diffusion:
  diffusion_steps:   1000
  ddim_steps:      10
  beta_schedule:    "linear" # Options: "linear", "cosine", 

  schedule_sampler:
    name: "uniform"
    max_steps: 1000
 
  clip_denoised:     true
  ddim:              false


amp:
  enabled:           false
  fp16_scale_growth: 1e-3

