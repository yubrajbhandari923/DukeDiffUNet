experiment:
  name: "DiffUNet_v2"
  version: 2.0
  description: "3.0 with Mobina Mixed Colon Dataset with colon_bowel prediction + convex hulled small bowel"
  tags: ["DiffUNet", "Mobina Mixed Colon Dataset"]
  debug: false
  hash: null

seed: 42

logging:
  use_aim:       true
  aim_repo:      "/home/yb107/cvpr2025/DukeDiffSeg/aim/"

data:
  # description:        "3d VLSM dataset for colon segmentation given by Lavsen, but with new combined labels with 22 classes"
  description:        "Mixed Mobina Colon Data: Refined Colons, Refined Male Female, and A graded dataset"
  num_classes:        13     # background + 12 organs
  colon_label:        1     # Label for colon segmentation
  cache_dir:          "/data/usr/yb107/colon_data/cache_mobina_mixed_colon_dataset"
  # cache_dir:          "/data/usr/yb107/colon_data/cache_tmp"
  orientation:        "RAS"
  pixdim:             [1.5, 1.5, 2.0]
  roi_size:           [96, 96, 96]
  # roi_size:           [128, 128, 128]
  # roi_size:           [192, 192, 192]
  # roi_size:           [256, 256, 256] # Adjusted for colon segmentation
  slice_axis:         -1
  
  train_jsonl:        "/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_train.jsonl"
  batch_size_per_gpu:         1
  num_workers_per_gpu:        4
  shuffle_train_data: true

  val_jsonl:          "/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_val.jsonl"
  val_batch_size:    1
  val_num_workers:    4

  save_data: false # Save all data during inference

# ───────────────────────────────────────────────────────────
# Training hyperparameters
training:
  device:              "cuda"
  epochs:              5000
  accumulate_grad_steps: 2
  save_dir:            "/home/yb107/cvpr2025/DukeDiffSeg/outputs"
  save_interval:       5      # epochs
  resume:              null # Path to checkpoint to resume from (set to null to not resume)
  start_epoch:         null
  num_gpus:          1      # Number of GPUs to use for training
  save_config_yaml:  true  # Save the config to a file for easy recreation
  inference_mode:    false

# ───────────────────────────────────────────────────────────
# Optimizer & scheduler
optimizer:
  name:       "AdamW"
  # lr: 1e-3
  # weight_decay: 1e-4
  lr:         2e-4
  weight_decay: 1e-3
  set_to_none: true  # Set optimizer parameters to None for memory efficiency

ema:
  enable: true
  ema_rate: 0.9999

lr_scheduler:
  # pick either LinearWarmupCosineAnnealingLR or any torch scheduler
  name:               "LinearWarmupCosineAnnealingLR"
  # only used by the warmup+cosine scheduler:
  warmup_epochs:      100
  max_epochs:         ${training.epochs}

# ───────────────────────────────────────────────────────────
# Evaluation & early stopping
evaluation:
  validation_interval:     40        # epochs
  validation_max_num_samples: 80
  metrics:               ["Mean Dice"]
  early_stopping:
    enabled: true
    patience: 20
  visualize:             true
  visualize_every_iter:  5

# ───────────────────────────────────────────────────────────
# Model architecture (easy to swap out a different 'model.params' block)
model:
  name: "DiffUNet"
  params:
    spatial_dims: 3
    in_channels:  ${constraint.model.params.in_channels} # Overridden 
    out_channels: ${task.model.params.out_channels} # Overridden 
    features: [32, 64, 128, 256, 512, 64]
    activation: ["LeakyReLU", {"negative_slope": 0.1, "inplace": False}]
    beta_schedule: "linear"
    use_spacing_info: false  # Use spacing information in the model

    dropout: 0.0
    use_checkpointing: true

# ───────────────────────────────────────────────────────────
# Diffusion-specific hyperparameters
diffusion:
  diffusion_steps:   1000
  ddim_steps:      10
  beta_schedule:    "linear" # Options: "linear", "cosine", 
  model_mean_type: "start_x" # Options: "eps", "start_x"
  guidance_scale: 1.0      # Scale for classifier-free guidance
  condition_drop_prob: 0.0  # Probability of dropping the condition for classifier-free guidance

  schedule_sampler:
    name: "uniform"
    max_steps: 1000
 
  clip_denoised:     true
  ddim:              false


amp:
  enabled:           false
  fp16_scale_growth: 1e-3

