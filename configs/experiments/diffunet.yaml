
# TODO:
#  - Add grad accumulation support and micro-batch support if necessary


# ───────────────────────────────────────────────────────────
# Basic experiment metadata
experiment:
  name: "DiffUNet"
  version: 1.0
  description: "Reproducing DiffUNet with Multi-Class Setup"
  tags: ["DiffUNet", "3D"]

# ───────────────────────────────────────────────────────────
# Global seed for reproducibility
seed: 42

# ───────────────────────────────────────────────────────────
# Logging & monitoring
logging:
  use_aim:       true
  aim_repo:      "/home/yb107/cvpr2025/DukeDiffSeg/aim/"
  log_interval:  10       # iterations
  aim_ui_port:   43800

# ───────────────────────────────────────────────────────────
# Data & DataLoader settings
data:
  description:        "Abdomen 1K dataset for segmentation"
  num_classes:        5 # background + 4 organs
  # cache_dir:          "/NAS/user_data/user/yb107/cache_dir"
  cache_dir:          "/data/usr/yb107/cache_3d"
  orientation:        "RAS"
  pixdim:             [1.5, 1.5, 2.0]
  roi_size:           [96, 96, 96]
  slice_axis:         -1
  
  train_jsonl:        "/home/yb107/cvpr2025/DukeDiffSeg/data/json/abdomen_1k_train_updated.jsonl"
  batch_size:         4
  num_workers:        8
  micro_batch_size:   1      # for gradient accumulation # Not implemented yet

  val_jsonl:          "/home/yb107/cvpr2025/DukeDiffSeg/data/json/abdomen_1k_val_updated.jsonl"
  val_batch_size:     1
  val_num_workers:    4

# ───────────────────────────────────────────────────────────
# Training hyperparameters
training:
  device:              "cuda"
  epochs:              5000
  accumulate_grad_steps: 1
  save_dir:            "/home/yb107/cvpr2025/DukeDiffSeg/outputs"
  save_interval:       5      # epochs
  resume:              null # Path to checkpoint to resume from (set to null to not resume)
  
# ───────────────────────────────────────────────────────────
# Optimizer & scheduler
optimizer:
  name:       "AdamW"
  lr:         2e-4
  weight_decay: 1e-3
  set_to_none: true  # Set optimizer parameters to None for memory efficiency

ema:
  enable: true
  ema_rate: 0.9999
  update_after_step: 1000
  update_every_step: true

lr_scheduler:
  # pick either LinearWarmupCosineAnnealingLR or any torch scheduler
  name:               "LinearWarmupCosineAnnealingLR"
  # only used by the warmup+cosine scheduler:
  warmup_epochs:      100
  max_epochs:         ${training.epochs}

# ───────────────────────────────────────────────────────────
# Evaluation & early stopping
evaluation:
  validation_interval:     10        # epochs
  validation_max_num_samples: 100 
  metrics:               ["dice", "hausdorff"]
  early_stopping:
    enable: true
    patience: 20
  visualize:             true
  visualize_every_iter:  10 
  save_predictions:      true
  save_pred_interval:    1

# ───────────────────────────────────────────────────────────
# Model architecture (easy to swap out a different 'model.params' block)
model:
  name: "DiffUNet"
  params: 
    spatial_dims: 3
    in_channels: 1
    out_channels: "${data.num_classes}"  # Number of classes for segmentation
    features: [32, 64, 128, 256, 512, 64]
    activation: ["LeakyReLU", {"negative_slope": 0.1, "inplace": False}]
    beta_schedule: "linear"

    dropout: 0.0
    use_checkpointing: true

# ───────────────────────────────────────────────────────────
# Diffusion-specific hyperparameters
diffusion:
  diffusion_steps:   1000
  ddim_steps:      10
  beta_schedule:    "linear" # Options: "linear", "cosine", 

  schedule_sampler:
    name: "uniform"
    max_steps: 1000
 
  clip_denoised:     true
  ddim:              false

# ───────────────────────────────────────────────────────────
# Mixed-precision & AMP settings
amp:
  enabled:           false
  fp16_scale_growth: 1e-3


